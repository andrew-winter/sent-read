# set up, signature functions
scrapes <- c("httr", "geniusr", "tidytext")
lapply(scrapes, require, character.only = TRUE)

bing <- get_sentiments("bing")
bing_positive_sent <- function(song) {
  song$sentiment[song$sentiment == "positive"] %>% length()
}
bing_negative_sent <- function(song) {
  song$sentiment[song$sentiment == "negative"] %>% length() *-1
}
get_unsorted <- function(token, stop = stop_wordsB) {
  lapply(1:length(token), function(q) {
    token[[q]] %>%
      lapply(anti_join, stop, "word") %>%
      lapply(anti_join, bing, "word") %>%
        bind_rows(.id = "song")
  } ) %>%
  bind_rows(.id = "album") %>%
    group_by(word) %>% count(sort = TRUE) %>% ungroup()
}
line_detect <- function(lyrics, str, just_one_album = FALSE) {
  if (just_one_album == FALSE) {
    result <- lapply(1:length(lyrics), function(xalbum) {
      lapply(1:length(lyrics[[xalbum]]), function(xsong) { 
        filter(lyrics[[xalbum]][[xsong]],
          str_detect(line, fixed(str, ignore_case = TRUE))
        ) %>% select(song_name, line)
      } ) %>% bind_rows(.id = "track")
    } ) %>% bind_rows(.id = "album")
  } else {
    result <- lapply(1:length(lyrics), function(ysong) {
      filter(lyrics[[ysong]],
        str_detect(line, fixed(str, ignore_case = TRUE))
      ) %>% select(song_name, line)
    } ) %>% bind_rows(.id = "track")
  }
  return(result)
}
plot_top_words <- function(join, number = 15) {
  result <- join %>% group_by(sentiment) %>% top_n(number) %>% ungroup() %>%
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(word, n, fill = sentiment)) + geom_col(show.legend = FALSE) +
      facet_wrap(~ sentiment, scales = "free_y") + coord_flip() +
      labs(x = NULL, y = "Contribution to sentiment")
  return(result)
}
simple_sent <- function(album, token, join, ...) {
  result <- lapply(1:length(album), function(q) {
    cbind.data.frame(album[[q]]$song_title,
      sapply(join[[q]], bing_positive_sent),
      sapply(join[[q]], bing_negative_sent),
      sapply(join[[q]], song_length),
      sapply(token[[q]], song_length)
    )
  } ) %>% bind_rows(.id = "album") %>% as_tibble()
  names(result) = c("album", "song", "pos", "neg", "slength", "tlength")
  return(result)
}
song_length <- function(song) { song %>% bind_rows %>% nrow }
stop_words_detect <- function(tokens, stop = stop_wordsB) { 
  lapply(1:3, function(x) {
    tokens[[x]] %>% lapply(inner_join, stop, "word") %>%
      bind_rows()
  } ) %>% bind_rows() %>% select(word) %>% group_by(word) %>%
    count(sort = TRUE) %>% ungroup()
}
top_words <- function(join) {
  lapply(1:length(join), function(x) {
    join[[x]] %>% bind_rows()
  } ) %>%
    bind_rows %>% group_by(word, sentiment) %>% count(sort = TRUE) %>% ungroup()
}

Chance_tracks <- c(29884, 150853, 446915)
token <- "Z12YlAUALbWknjemUTrwHSnGkbbCpbprSqoVbt7yXhjz0sDX_v3pRnNPTq8k2Ldm"


# scrape albums and lyrics, tokenize 
Chance_albums <- lapply(Chance_tracks, scrape_tracklist)
Chance_lyrics <- lapply(c(1:3), function(x) { 
  Chance_albums[[x]]$song_lyrics_url %>% lapply(scrape_lyrics_url) } )
Chance_tokens <- lapply(c(1:3), function(y) { 
  Chance_lyrics[[y]] %>% lapply(unnest_tokens, word, line) } )

Chance_orig_join <- lapply(1:3, function(x) {
  Chance_tokens[[x]] %>% lapply(inner_join, bing, "word") } )

Chance_orig_join %>% top_words()
Chance_orig_join %>% top_words() %>% plot_top_words()
simple_sent(Chance_albums, Chance_tokens, Chance_orig_join)

# insert 'simple_sent()' visualization and discussion here
# but what if we had a different bing set?


# another problem: should 'get_unsorted()' use stop_words or stop_wordsB
  # There are potentially a ton of stop_words that aren't correct
    # ex: "good" is a stop_word for no reason at all
# I think I'd have to remove it from stop_words AND add it to custom_words
# Story note: I saw "kind" in stop_words, removed it, thought about it, then
  # used 'line_detect()' to see how it's used. Turns out, 2/3 of the instances
  # are "kinda" or "kind of x", 1/6 is "one of a kind", and only one is "kind"
  # like generous

# 0. basic sentiment analysis (probably with stop_words), then 'top_words()'
# 1. use 'stop_words_detect()'
  # ensure that stop_words aren't stopping anything they shouldn't (ex. "good")
  # assess via 'line_detect()', make custom stop_words via 'anti_join()'
  # commentary: "Look which words were stopped! Decreased sentiment!"
stop_words_rm <- tribble(~word, "good", "great", "better", "greatest",
  "together", "please", "best")
stop_wordsB <- anti_join(stop_words, stop_words_rm, "word")

# 2. use 'get_unsorted()'
  # determine if any words should have sentiment or should be stopped
  # commentary: "look at all the words that aren't in the bing dataset!"
Chance_custom_words <- tribble(~word, ~sentiment,
                       "kisses", "positive",
                       "special", "positive")
Chance_lexicon <- bind_rows(bing, Chance_custom_words)

# 3. make join (?)
# 4. use 'top_words()' and 'line_detect()'
  # commentary: "look at all the words that were incorrectly (for our purposes)
    # in the bing dataset!"
# jam, and miss, maybe


Chance_tokens %>% get_unsorted
line_detect(Chance_lyrics, "juice")
line_detect(Chance_lyrics, "god")
line_detect(Chance_lyrics, "single")
line_detect(Chance_lyrics, "day")
line_detect(Chance_lyrics, "kisses")
line_detect(Chance_lyrics, "blessings")
line_detect(Chance_lyrics, "forever")

Chance_orig_join %>% top_words
line_detect(Chance_lyrics, "problem")
line_detect(Chance_lyrics, "miss")





# important to remember: stop_words_rm might have some words that aren't
  # bing words-- they were correctly removed from stop_words, but not
  # necesssarily added to the lexicon
# NOTE: can't simply add stop_words_rm to Chance_lexicon, because if there
  # are two instances of the same word, it'll be double counted
# Have got to think of a more efficient way to undertake the fix steps
# Going to be a mission regardless




Chance_custom_stops <- tribble(~word, ~lexicon,
                               "like", "custom")
Chance_stop_words <- bind_rows(stop_wordsB, Chance_custom_stops)

Chance_custom_join <- lapply(1:3, function(x) {
  Chance_tokens[[x]] %>% lapply(anti_join, Chance_stop_words, "word") %>%
  lapply(inner_join, Chance_lexicon, "word") } )

Chance_custom_join %>% top_words()



orig <- simple_sent(Chance_albums, Chance_tokens, Chance_orig_join) %>%
  mutate(posi = 100 * pos/slength, negi = 100* neg/slength)
custom <- simple_sent(Chance_albums, Chance_tokens, Chance_custom_join) %>%
  mutate(posi = 100 * pos/slength, negi = 100* neg/slength)

custom$song <- factor(custom$song, levels = unique(as.character(custom$song)))
custom$album <- factor(custom$album, levels = unique(as.character(custom$album)))
custom %>% ggplot(aes(x = song)) + 
  geom_col(aes(y = posi, fill = album), linetype = 1, alpha = 0.8,
    color = "black") +
  geom_col(aes(y = negi), linetype = 1, alpha = 0.6, color = "black")

orig$song <- factor(orig$song, levels = unique(as.character(orig$song)))
orig$album <- factor(orig$album, levels = unique(as.character(orig$album)))
orig %>% ggplot(aes(x = song)) + 
  geom_col(aes(y = posi, fill = album), linetype = 1, alpha = 0.8) +
  geom_col(aes(y = negi), linetype = 1, alpha = 0.6)







# 'like' is very prevalent in "The Big Day", 'love' is very
# prevalent in "The Big Day" and "Acid Rap" but not "Coloring Book", and oddly
# enough, 'die' is ONLY seen in "The Big Day", somehow


Gambino_tracks <- c(11571, 47467, 156650)
Gambino_albums <- lapply(Gambino_tracks, scrape_tracklist)
Gambino_lyrics <- lapply(c(1:3), function(x) { 
  Gambino_albums[[x]]$song_lyrics_url %>% lapply(scrape_lyrics_url)})
Gambino_tokens <- lapply(c(1:3), function(y) { 
  Gambino_lyrics[[y]] %>% lapply(unnest_tokens, word, line) })

Gambino_orig_join <- lapply(1:3, function(x) {
  Gambino_tokens[[x]] %>% lapply(inner_join, bing, "word") } )


Tyler_tracks <- c(10761, 10762, 18842, 352640, 526062)
Tyler_albums <- lapply(Tyler_tracks, scrape_tracklist)
Tyler_lyrics <- lapply(c(1:5), function(x) { 
  Tyler_albums[[x]]$song_lyrics_url %>% lapply(scrape_lyrics_url)})
Tyler_tokens <- lapply(c(1:5), function(y) { 
  Tyler_lyrics[[y]] %>% lapply(unnest_tokens, word, line) })


# old functions
Chance_bing_words <- lapply(c(1:3), function(b) {
  bind_rows(Chance_tokens[[b]]) } ) %>% lapply(inner_join, bing, "word")
Chance_bing_counts <- lapply(c(1:3), function(c) {
  count(Chance_bing_words[[c]], word, sentiment, sort = TRUE) } )
top_album_words <- function(join) {
  lapply(1:length(join), function(x) {
    join[[x]] %>% bind_rows() %>% group_by(word, sentiment) %>%
    count(sort = TRUE) %>% ungroup() } )
}
plot_top_album_words <- function(join, number = 10) {
  bind_rows(join, .id = "album") %>% group_by(sentiment, album) %>% 
  top_n(number) %>% ungroup() %>% mutate(word = reorder(word, n)) %>%
    ggplot(aes(word, n, fill = album)) + geom_col() +
      facet_wrap(album ~ sentiment, scales = "free_y") + coord_flip() +
      labs(y = "Contribution to sentiment", x = NULL)
}
